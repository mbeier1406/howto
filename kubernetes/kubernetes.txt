#
# Kubernetes
#

# Installationen
  - Rancher's next-generation Kubernetes rke2
  echo "Y3VybCAtc2ZMIGh0dHBzOi8vZ2V0LnJrZTIuaW8gfCBzaCAtCgogICAgc3lzdGVtY3RsIGVuYWJsZSAtLW5vdyBya2UyLXNlcnZlci5zZXJ2aWNlCgogICAgY3VybCAtTE8gaHR0cHM6Ly9kbC5rOHMuaW8vcmVsZWFzZS92MS4yMS4wL2Jpbi9saW51eC9hbWQ2NC9rdWJlY3RsCgogICAgY2htb2QgK3gga3ViZWN0bAoKICAgIG12IC9yb290L2t1YmVjdGwgL3Vzci9iaW4vCgogICAga3ViZWN0bCAtLWt1YmVjb25maWcgL2V0Yy9yYW5jaGVyL3JrZTIvcmtlMi55YW1sIGdldCBhbGwgLW4ga3ViZS1zeXN0ZW0KCiAgICBleHBvcnQgS1VCRUNPTkZJRz0vZXRjL3JhbmNoZXIvcmtlMi9ya2UyLnlhbWwKCiAgICBlY2hvICJQbGVhc2UgcnVuIHRoZSBmb2xsb3dpbmcgY29tbWFuZCBmb3IgY29ubmVjdGluZyB0byB0aGUgY2x1c3RlcjoiCgogICAgZWNobyAiZXhwb3J0IEtVQkVDT05GSUc9L2V0Yy9yYW5jaGVyL3JrZTIvcmtlMi55YW1sIg==" | base64 --decode | bash
  export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
  - Lightweight Kubernetes: k3s
  curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.24.4+k3s1 sh - # Installation und Start k3s
  export KUBECONFIG=/etc/rancher/k3s/k3s.yml # Client Konfigurationsdatei kubeconfig
  ln -sf /etc/rancher/k3s/k3s.yaml ~/.kube/config
  - Kubernetes mit Terminal UI: k9s
  # Installationsanleitung in https://github.com/derailed/k9s#installation
  curl -sS https://webinstall.dev/k9s | bash # Beispiel ueber Webinstall
  export PATH=/root/.local/bin:${PATH}
  k9s # :q fuer Ende
  k9s --kubeconfig ...yaml # Oder export KUBECONFIG=...

  # Cluster Administration
  # * Pruefen, ob der Cluster laeuft
    kubectl get nodes # muessen Status "Ready" haben
  # * RBAC und Authentifizierung
  #   User werden lokal verwaltet oder extern ueber LDAP/AD (je nach Cluster Setup)
  #   Regel (Rule) => Gruppierung Zugriffsberechtigungen auf (eine Gruppe von) Ressourcen und deren Scope: Ressource, Verb, apiGroup
  #     Ressource: Pod, Deployment, ... (s. u.)
  #     Verb: get, list, watch, create, update, scale, delete, ...
  #     ApiGroup: "" (Core), apps, batch, extensions, ...
  #   Rolle => Gruppierung von Regeln (vordefiniert zB "cluster-admin" fuer alle Ressourcen im Cluster, "admin" fuer alle im Namespace)
        kubectl create role/clusterrole ...
        kubectl get/describe role/clusterrole ...
        kubectl create rolebinding/clusterrolebinding
  #   Pruefen, ob ein bestimmter User ein Recht hat:
        kubectl auth can-i <verb> <resource> --namespace <name> --as <username>/--group[..., ...]
  #   User/Gruppen => werden Rollen zugewiesen
  #   - User Typen:
  #     + ServcieAccount: Gehoert zum Pod/Deployment fuer den Zugriff auf den API-Service mit entsprechenden Berechtigungen (Standard: default)
  #       https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
        kubectl get serviceaccount default --namespace ...
  #       Im Pod (oder Deployment) kann der ServiceAccount gewechset werden mit "spec.serviceAccountName"
  #   - User Authentication:
  #     + Basic Authentication: User/Password
  #     + Token-based Authentication: Eindeutige Zeichenkette des Authentication Servers
  #     + Client Certificate Authentication: Digitales Client-Zertifikat
  #   - Rollen:
  #     + ClusterRole: betrifft die Ressourcen im gesamten Cluster, unabhaengig vom Namespace (z. B. "node")
  #     + Role: betrifft nur Ressourcen in einem bestimmten Namespace

  # Imperative Kommandos
  kubectl run nginx-pod --image nginx:1.12.0 # Pod im default Namespace starten
  kubectl run <Podname> --image <Image> -- <Command> # spezielles Kommando im Pod ausfuehren
  kubectl get pods -ocustom-columns="NAME:.metadata.name,IMAGE:.spec.containers[0].image,NAMESPACE:.metadata.namespace" # bestimmte Spalten anzeigen
  kubectl create namespace localns; kubectl run redis --image redis:alpine --labels app=db --namespace localns --port 8080 # im eigenen Namespace
  kubectl create deployment web --image jcdemo/flaskapp --replicas 4 # Deployment mit vier Replicas
  kubectl run httpd --image httpd:alpine --port 80 --expose # Pod mit Service (Cluster IP)
  kubectl run ... --dry-run=client --output=<Format (yaml, json, ...)> > <Dateiname> # Pod-Definition als Datei speichern

  # Objekte
  # Kubernetesobjekte wie zB Pods (record of intend) mit Sepc/Status ueber den API-Server (zB ueber kubectl) pflegen:
  # - containerisierte Anwendungen auf den Nodes
  # - welche Ressourcen fuer sie zur Verfuegung stehen
  # - Richtlinien fuer Neustarte, Upgrades, Fehlertoleranz, ...
  # Objektbeschreibungen: https://kubernetes.io/docs/reference/kubernetes-api/
  # Objektinfos:
  # - Metadata
  # - Spec = desired state
  # - Status = aktueller Stand
  # Definition der drei: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md
  # * Pods
    kubectl apply --filename ngnix-pod.yaml # Pod anlegen
    kubectl describe pod ... # Infos zum Pod
    kubectl get pods # Pods anzeigen
    kubectl delete --filename ngnix-pod.yaml # Pod wieder loeschen
  # * Probes: laufen die Container?
  #   Die Restart-Policy betrifft immer den gesamten Pod! Bei Deployments/DaemonSets/StatefulSets immer "Always"
  #   - Liveness: "Lebt" der Container? Neustart des Containers bei Fehlern.
    kubectl apply --filename liveness-pod.yaml
  #   - Readiness: Verarbeitet der Container Anfragen? Keine weiteren werden dorthin delegiert bei Fehlern.
    kubectl apply --filename readiness-pod.yaml
  #   - Startup: Ist der Container bereits hochgefahren? Vorher keine Liveness/Readiness Proben
    kubectl apply --filename startupprobe-pod.yaml
  # Image eines Containers anzeigen:
    kubectl get pod <Name> --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'
  # * Deployments: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
  #   Managen ReplicaSets (automatischer Neustart usw.) = mehrere identische Pods:
  #   - Verschiedene Updatestrategien (rolling update usw.) und rollback zu aelterem Stand
  #   - Automatische Skalierung ueber Anzahl der Pods
  #   - Automatischer Neustart
    kubectl apply --filename nginx-deployment.yaml # Deployment anlegen, yaml-Date aendern erneut ausfuehren = Update
    kubectl get deployments
    watch kubectl get pods -o wide
  # * ReplicaSets
  #   Automatisch wird ein ReplicaSet mit angelegt
  #   Bei Updates bleibt ein Deployment, aber mehrere ReplicaSets (nicht-aktive mit 0 desired Pods!)
    kubectl get replicasets
  #   Skalierung: ReplicaSet aendern
    watch kubectl get pods # "desired state" des aktiven ReplicaSets anpassen
    kubectl scale deployment/nginx-deployment --replicas=...
    kubectl patch deployment/nginx-deployment '{"spec":{"replicas":<Number>}}' # alternativ
    kubectl delete pod <Pod-Name>; watch kubectl get replicasets # wird wieder auf die spezifizierte Anzahl Pods geaendert
  # * Namespaces
  #   Fuer mulit-user Szenarios, Workloads trennen
  #   Produktion absichern: https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/
    kubectl create namespace ... # fuer einen privaten Workload
    kubectl get namespaces
    kubectl get pods --namespace=... # zB kube-system fuer interne Pods
    kubectl get pods --all-namspaces # alle Pods in allen Namespaces
  # * Logging
  #  Nur fuer Pods
    kubectl apply --filename counter-pod.yaml # Pod mit Ausgaben auf stdout
    kubectl logs counter --follow
  # Command execution (nur fuer Pods)
  kubectl exec --tty --stdin <Pod> -- <Kommando>

  # Resourcen
  # * Secrets
  #   Im Standard unverschluesselt im etcd gespeichert
  #   => Loesung: 1. Definition RBAC-Rolle 2. Zugriff auf spezielle Container einschraenken 3. externe Speicher
  #   Typen:
  #   - opaque: Allgemein fuer Text
  #   - docker-registry: Zugriff auf private Docker Registries
  #   - tls: private Schluessel und Zertifikate
  #   - ssh: private Schluessel, fingerprints
  #   - service-account-token: Zugriff auf Serviceaccounts ueber den API-Server
    kubectl create secret <Typ> <Name> <Daten>
    kubectl get secrets # Spalte DATA: Anzahl Eintraege
    kubectl get secret <Secret-Name> --output yaml
    kubectl describe secret <Secret-Name>
    # Opaque
    kubectl create secret generic geheim-opaque --from-literal=user=... --from-literal=pass=...
    # alternativ --from-file=<dir> oder kubectl apply --filename opaque-secret.yaml
    kubectl apply --filename pod-opaque-secret.yaml # Testen...
    kubectl exec --tty --stdin pod-opaque-secret -- /bin/bash
    $ echo $user $pass
    # TLS
    openssl req -x509 -newkey rsa:4096 -nodes -keyout tls.key -out tls.crt -subj "/CN=golkonda.de" # Key und Zertifikat
    kubectl create secret tls geheim-tls --cert=tls.crt --key=tls.key # Test analog Opaque
    # docker-registry: wird auf dem Node fuer das Kubelet zur Anmeldung an der Image-Registry benoetigt
    kubectl create secret docker-registry geheim-dockerregistry  \
    --docker-server=DOCKER_REGISTRY_SERVER \
    --docker-username=DOCKER_USER \
    --docker-password=DOCKER_PASSWORD \
    --docker-email=DOCKER_EMAIL
    kubectl apply --filename pod-dockerregistry-secret.yaml # laedt das Image von der zuvor definierten Registry
    # Da Secrets von jedem API/etcd-User, Pod, Deployment, ... ausgelesen werden koennen, externer Speicher mit mehr Sicherheit:
    # - https://www.vaultproject.io/docs/
    # - https://aws.amazon.com/secrets-manager/
  # * ConfigMaps
  #   Konfigurationsvariablen fuer Pods bereitstellen
    kubectl get configmap <Name> # aus metadata/name
    kubectl describe configmap <Name>
    # Bereitstellen als Umgebungsvariablen => werden bei Updates NICHT automatisch aktualisiert
    kubectl apply --filename=config-map.yaml
    kubectl apply --filename=config-map-pod.yaml
    # Bereitstellen als  Konfigurationsdateien => werden bei Updates automatisch aktualisiert
    kubectl apply --filename=config-map-file.yaml
    kubectl apply --filename=config-map-file-pod.yaml
  # * Service
  #   https://kubernetes.io/docs/concepts/services-networking/service/: Anwendung im Cluster bekannt machen
    kubectl apply --filename deployment-mongodb.yaml # Beispiel MongoDB mit Gaestebuch
    kubectl apply --filename service-mongodb.yaml
    kubectl apply --filename deployment-guestbook.yaml
    kubectl apply --filename deployment-guestbook.yaml
    kubectl get service frontend
    curl http://localhost:30080
  # * Ingress
  #    https://kubernetes.io/docs/concepts/services-networking/ingress/: Anwendungen veroeffentlichen ueber Reverse Proxy
  #    Alternative zum Service mit NodePort
    kubectl apply --filename ingress-guestbook.yaml
    kubectl get ingress
    curl http://localhost:80
  # * PersistentVolume
  #   Anforderung (PVC): https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims
  #   Manuell bereitgestellt (PV): https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  #   Automatisch ueber StorageClass: https://kubernetes.io/docs/concepts/storage/storage-classes
  #   Unabhaengig von Pods, Daten "ueberleben" auch deren Loeschung
    kubectl apply --filename pvc.yaml # Anforderung des Volumes mit Namen PVC-NAME
    kubectl apply --filename pv.yaml # Verwnedung des Volumes mit Namen PVC-NAME
    kubectl get/describe pv/pvc

  # Jobs/CronJobs
  # Jobs: ein oder mehrere Pods starten und Erolg sicherstellen: fuer kurze Prozesse/Batch
  # CronJob: Jobs mit Zeitsteuerung
  kubectl apply -f job-busybox.yaml
  kubectl get jobs
  kubectl describe job <Jobname> # hier: busybox-job
  kubectl get pods -ljob-name=<Jobname>
  kubectl logs --selector=job-name=<Jobname>
  kubectl apply --filename cronjob-busybox.yaml
  kubectl get cronjobs
  kubectl describe cronjob <Cronjobname> # hier: busybox-cronjob
  watch kubectl get pods --selector=job-name=<Cronjobname>

  # Static Pods
  # Statische Pods (an einen Knoten gebunden) sind vom CONTROLLER-Typ Node und werden direkt vom kubelet (nicht API-Server) verwaltet
  # Ablegen der yaml-Datei im Manifest-Verzeichnis (Kublet Konfiguration) des Knotens
  # Beispiel static-pod-manifest.yaml
  kubectl get pods --all-namespaces --output custom-columns="NAME:.metadata.name,CONTROLLER:.metadata.ownerReferences[].kind"

  # Application Logging
  # kubectl logs gilt nur fuer gerade laufende Pods, "ueberlebt" den Neustart nicht! => Log Aggregierung
  # log aggregation: logs -> Promtail -> Grafana loki (Central Log Collector) -> Grafana (Rendering)
  # Helm installieren:
    curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
    export KUBECONFIG=...
    helm repo add grafana https://grafana.github.io/helm-charts
    helm repo update
    helm upgrade --install loki grafana/loki-stack  --set grafana.enabled=true,prometheus.enabled=true,prometheus.alertmanager.persistentVolume.enabled=false,prometheus.server.persistentVolume.enabled=false
    kubectl apply --filename=grafana-dashboard.yaml
  # Loki-Adresse im Cluster: http://loki:3100/
  # Kennwort ermitteln:
    kubectl get secret loki-grafana --output=jsonpath="{.data.admin-password}" | base64 --decode ; echo
  # Grafana Dashboard ueber http://localhost:80/login
  # Ueberpruefen, ob Loki als Datasource vorhanden ist: Configuration -> Data sources
  # Wenn fehlt: Add Datasource (Choose Loki from the list); Enter http://loki:3100/ as URL
  # Ergebnisse der Pods in Explore -> {<key>=<value>} (.metatdata.labels) -> <SHIFT>-<ENTER>

  # DaemonSets
  # Kopien eines bestimmten Pods sollen auf mehreren/allen Knoten im Cluster laufen (mehere bei node labels)
  # Benoetigt fuer Monitoring, Logging, Ingress usw. Beim Einfuegen eines neuen knoten in den Cluster wird eine Kopie aus dem DaemonSet erzeugt.
    kubectl get daemonsets --all-namespaces
  # Definiert wie Deployment (kind: DaemonSet) ohne replicas, strategy und status Attribute
    kubectl create deployment ...-ds --image ... --dry-run=client --output=yaml > deployment-template.yaml # editieren
    kubectl apply --filename=deployment-template.yaml
    kubectl get daemonsets
    kubectl get pods
  # Sollen nur bestimmte Knoten fuer den DaemonSet gelten, muessen diese gelabeld werden:
    kubectl label nodes <NodeName> <key>=<value>
  # In spec.template.spec.nodeSelector:
       nodeSelector:
        <key>: <value>

  # StatefulSets
  # Fuer Stateful Applications in verteilten Systemen (also stabilen Namen, Speicher usw.)
  # Insbesondere kann die Updatestrategie (RollingUpdate, OnDelete) fuer die Pods festgelegt werden.
    kubectl apply --filename=statefullset-example.yaml # Ein Beispiel
  # Pods werden einer nach dem anderen gestartet, sobald der Pod im Status "Running" und "Ready" (readiness)
  # Pod-Name und Hostname erhalten den Index als Extension: ...-0, ...-1, ...
  # Die Namen "ueberleben" einen Neustart des Pods (auch auf einem anderen Knoten), _nicht_ die IP!
  # Gilt auch fuer den DNS-namen des Services:
    kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm
    nslookup <StatefulSet-Name>-0.<Service-Name>
    nslookup <StatefulSet-Name>-1.<Service-Name>
  # Pods loeschen und kontrollieren, wie sie in der Reihe wieder neu gestartet werden und die Hostnames gleich geblieben sind:
    kubectl delete pod --selector=<key>=<value>
    kubectl get pods --selector=<key>=<value> --watch=true
  # Persistent/Stable Storage PV/PVC, Skalierung wie bei ReplicaSets
  # Updates:
    kubectl patch statefulset <Name> --patch '{"spec":{"updateStrategy":{"type":"..."}}}'
    kubectl patch statefulset <Name> --type='json' --patch='[{"op": "replace", "path": "/spec/template/spec/containers/<Index>/image", "value":"..."}]'
    kubectl get pods --selector=<key>=<value> --watch=true
  # Loeschen: bei nicht cascadierend "ueberleben" die Pods!
    kubectl delete statefulset <Name> --cascade=false

  # Helm
  # https://helm.sh/: Applikationen aus Repositories im Cluster deployen
  # Wird auf dem Client installiert, kommuniziert mit dem API-Server mit dem Cluster: https://helm.sh/docs/intro/install/
  curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash # Installation
  helm version --client
  # Installation mit Datei values.yaml <key>: <value> ersetzt {{ .Values.<key> }} durch <value>
  helm install <Name> --filename=<values.yaml> <Installationsverzeichnis>
  helm upgrade <Name> --set <key>=<value>
  helm list
  helm rollback <Name> <Revision>
  helm history <Name>
  helm uninstall <Name1> [<Name2>] ...

